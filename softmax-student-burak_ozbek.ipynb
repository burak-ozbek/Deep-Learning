{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP541 - LAB #2\n",
    "## Classifying MNIST digits with a softmax classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you will implement a softmax classifier to predict the digit presented in a given image.\n",
    "We will use the MNIST dataset for this task. Please first skim through the notebook. Then complete the following steps mentioned in the main function:\n",
    "\n",
    "1. minibatch\n",
    "2. init_params\n",
    "3. forward and backward propagation\n",
    "    * softmax_forw\n",
    "    * softmax_back_and_loss\n",
    "4. grad_check\n",
    "5. train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we install **Knet** and **MLDatasets** only to be able to use MNIST dataset and to import some functions for testing purposes.\n",
    "Please execute this cell and enter `y` to download the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   Updating\u001b[22m\u001b[39m registry at `C:\\Users\\burak\\.julia\\registries\\General`\n",
      "\u001b[32m\u001b[1m  Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m BufferedStreams ─── v1.0.0\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m FixedPointNumbers ─ v0.8.4\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m HDF5 ────────────── v0.14.3\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m MLDatasets ──────── v0.5.3\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m Blosc_jll ───────── v1.14.3+1\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m GZip ────────────── v0.5.1\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m BinDeps ─────────── v1.0.2\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m Lz4_jll ─────────── v1.9.2+2\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m HDF5_jll ────────── v1.12.0+1\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m ColorTypes ──────── v0.10.9\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m URIParser ───────── v0.4.1\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m OpenSSL_jll ─────── v1.1.1+6\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m nghttp2_jll ─────── v1.40.0+2\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m BinaryProvider ──── v0.5.10\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m URIs ────────────── v1.1.0\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m LibSSH2_jll ─────── v1.9.0+3\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m IniFile ─────────── v0.5.0\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m Blosc ───────────── v0.7.0\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m HTTP ────────────── v0.9.1\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m DataDeps ────────── v0.7.6\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m Zstd_jll ────────── v1.4.5+2\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m LibCURL_jll ─────── v7.70.0+2\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m MAT ─────────────── v0.9.2\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m p7zip_jll ───────── v16.2.0+3\n",
      "\u001b[32m\u001b[1mUpdating\u001b[22m\u001b[39m `C:\\Users\\burak\\.julia\\environments\\v1.5\\Project.toml`\n",
      " \u001b[90m [eb30cadb] \u001b[39m\u001b[92m+ MLDatasets v0.5.3\u001b[39m\n",
      "\u001b[32m\u001b[1mUpdating\u001b[22m\u001b[39m `C:\\Users\\burak\\.julia\\environments\\v1.5\\Manifest.toml`\n",
      " \u001b[90m [9e28174c] \u001b[39m\u001b[92m+ BinDeps v1.0.2\u001b[39m\n",
      " \u001b[90m [b99e7846] \u001b[39m\u001b[92m+ BinaryProvider v0.5.10\u001b[39m\n",
      " \u001b[90m [a74b3585] \u001b[39m\u001b[92m+ Blosc v0.7.0\u001b[39m\n",
      " \u001b[90m [0b7ba130] \u001b[39m\u001b[92m+ Blosc_jll v1.14.3+1\u001b[39m\n",
      " \u001b[90m [e1450e63] \u001b[39m\u001b[92m+ BufferedStreams v1.0.0\u001b[39m\n",
      " \u001b[90m [3da002f7] \u001b[39m\u001b[92m+ ColorTypes v0.10.9\u001b[39m\n",
      " \u001b[90m [124859b0] \u001b[39m\u001b[92m+ DataDeps v0.7.6\u001b[39m\n",
      " \u001b[90m [53c48c17] \u001b[39m\u001b[92m+ FixedPointNumbers v0.8.4\u001b[39m\n",
      " \u001b[90m [92fee26a] \u001b[39m\u001b[92m+ GZip v0.5.1\u001b[39m\n",
      " \u001b[90m [f67ccb44] \u001b[39m\u001b[92m+ HDF5 v0.14.3\u001b[39m\n",
      " \u001b[90m [0234f1f7] \u001b[39m\u001b[92m+ HDF5_jll v1.12.0+1\u001b[39m\n",
      " \u001b[90m [cd3eb016] \u001b[39m\u001b[92m+ HTTP v0.9.1\u001b[39m\n",
      " \u001b[90m [83e8ac13] \u001b[39m\u001b[92m+ IniFile v0.5.0\u001b[39m\n",
      " \u001b[90m [deac9b47] \u001b[39m\u001b[92m+ LibCURL_jll v7.70.0+2\u001b[39m\n",
      " \u001b[90m [29816b5a] \u001b[39m\u001b[92m+ LibSSH2_jll v1.9.0+3\u001b[39m\n",
      " \u001b[90m [5ced341a] \u001b[39m\u001b[92m+ Lz4_jll v1.9.2+2\u001b[39m\n",
      " \u001b[90m [23992714] \u001b[39m\u001b[92m+ MAT v0.9.2\u001b[39m\n",
      " \u001b[90m [eb30cadb] \u001b[39m\u001b[92m+ MLDatasets v0.5.3\u001b[39m\n",
      " \u001b[90m [458c3c95] \u001b[39m\u001b[92m+ OpenSSL_jll v1.1.1+6\u001b[39m\n",
      " \u001b[90m [30578b45] \u001b[39m\u001b[92m+ URIParser v0.4.1\u001b[39m\n",
      " \u001b[90m [5c2747f8] \u001b[39m\u001b[92m+ URIs v1.1.0\u001b[39m\n",
      " \u001b[90m [3161d3a3] \u001b[39m\u001b[92m+ Zstd_jll v1.4.5+2\u001b[39m\n",
      " \u001b[90m [8e850ede] \u001b[39m\u001b[92m+ nghttp2_jll v1.40.0+2\u001b[39m\n",
      " \u001b[90m [3f19e933] \u001b[39m\u001b[92m+ p7zip_jll v16.2.0+3\u001b[39m\n",
      "\u001b[32m\u001b[1m   Building\u001b[22m\u001b[39m HDF5 ────→ `C:\\Users\\burak\\.julia\\packages\\HDF5\\d0V7K\\deps\\build.log`\n",
      "\u001b[32m\u001b[1m   Building\u001b[22m\u001b[39m DataDeps → `C:\\Users\\burak\\.julia\\packages\\DataDeps\\jrlAW\\deps\\build.log`\n",
      "┌ Info: Precompiling MLDatasets [eb30cadb-4394-5ae3-aed4-317e484a6458]\n",
      "└ @ Base loading.jl:1278\n",
      "┌ Info: Adding MLDatasets\n",
      "└ @ Main In[1]:5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This program has requested access to the data dependency MNIST.\n",
      "which is not currently installed. It can be installed automatically, and you will not see this message again.\n",
      "\n",
      "Dataset: THE MNIST DATABASE of handwritten digits\n",
      "Authors: Yann LeCun, Corinna Cortes, Christopher J.C. Burges\n",
      "Website: http://yann.lecun.com/exdb/mnist/\n",
      "\n",
      "[LeCun et al., 1998a]\n",
      "    Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner.\n",
      "    \"Gradient-based learning applied to document recognition.\"\n",
      "    Proceedings of the IEEE, 86(11):2278-2324, November 1998\n",
      "\n",
      "The files are available for download at the offical\n",
      "website linked above. Note that using the data\n",
      "responsibly and respecting copyright remains your\n",
      "responsibility. The authors of MNIST aren't really\n",
      "explicit about any terms of use, so please read the\n",
      "website to make sure you want to download the\n",
      "dataset.\n",
      "\n",
      "\n",
      "\n",
      "Do you want to download the dataset from [\"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\", \"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\", \"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\", \"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\"] to \"C:\\Users\\burak\\.julia\\datadeps\\MNIST\"?\n",
      "[y/n]\n",
      "stdin> y\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Pkg; for p in [\"MLDatasets\"]; Pkg.add(p); end\n",
    "using Printf, Random, Test, Statistics\n",
    "using MLDatasets: MNIST\n",
    "\n",
    "@info \"Adding MLDatasets\"\n",
    "@test size.(MNIST.traindata(Float32)) == ((28, 28, 60000), (60000,))\n",
    "@test size.(MNIST.testdata(Float32)) == ((28, 28, 10000), (10000,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "Here you should implement `minibatch` function which takes raw input array `x` and gold labels arrays `y` and splits them into mini batches to be processed.\n",
    "Hints: you can check arrays size with `size` function and reshape them using `reshape`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Testing minibatch function\n",
      "└ @ Main In[3]:23\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    minibatch(x, y, bs=100)\n",
    "\n",
    "Return a list of minibatches [(xi,yi)...] given data tensors x, y and batchsize.\n",
    "\n",
    "The last dimension of x and y give the number of instances and should be equal.\n",
    "\"\"\"\n",
    "function minibatch(x, y, bs=100)\n",
    "    data = Any[]\n",
    "\n",
    "    # Your code here\n",
    "    for i=1:bs:size(x)[3]\n",
    "        \n",
    "        j=min(i+bs-1,size(x)[3])\n",
    "\n",
    "        push!(data, (x[:,:,i:j], y[i:j]))\n",
    "\n",
    "    end\n",
    "\n",
    "    return data\n",
    "end\n",
    "\n",
    "@info \"Testing minibatch function\"\n",
    "@test mean(minibatch(MNIST.testdata(Float32)...)[1][1]) ≈ 0.11988331\n",
    "@test size.(minibatch(MNIST.testdata(Float32)...)[100]) == ((28, 28, 100), (100,))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "Here you should implement `init_params` function which will be used to produce the initial values of the weights.\n",
    "Hints : look up `randn` and `zeros` functions using `@doc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Testing init_params function\n",
      "└ @ Main In[4]:15\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    init_params(ninputs, noutputs)\n",
    "\n",
    "Return a tuple of randomly generated W(sampled from N(0, 1e-3)) and b(must be zeros vector) params of softmax.\n",
    "\"\"\"\n",
    "function init_params(ninputs::Int, noutputs::Int)\n",
    "    # Your code here\n",
    "    W= 1e-3* randn(noutputs,ninputs)\n",
    "    b=zeros(noutputs,1)     \n",
    "    return W,b\n",
    "end\n",
    "Random.seed!(1)\n",
    "W, b = init_params(150, 100)\n",
    "\n",
    "@info \"Testing init_params function\"\n",
    "@test mean(W) ≈ 2.6869455422978772e-6\n",
    "@test size(b) == (100, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.1\n",
    "\n",
    "This function will takes three arguments, model weights `w`, `b` and a single minibatch input data `x`, applies the affine transformation and softmax function and returns predicted probabilities.\n",
    "After applying the affine transformation by multplying the input minibatch with the weights and adding the bias vector, you need to implement softmax function as follows:\n",
    "\n",
    "\\begin{eqnarray}{\\displaystyle P(y=j\\mid \\mathbf {x} )={\\frac {e^{\\mathbf {x} \\mathbf {w} _{j} + \\mathbf {b} _{j}}}{\\sum _{k=1}^{K}e^{\\mathbf {x} \\mathbf {w} _{j} + \\mathbf {b} _{k}}}}}\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Testing softmax_forw\n",
      "└ @ Main In[5]:20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    softmax_forw(W, b, x)\n",
    "\n",
    "Return the predicted probabilities of softmax function\n",
    "\"\"\"\n",
    "function softmax_forw(W, b, x)\n",
    "    # Your code here   \n",
    "    probs = W*x .+ b\n",
    "    #probs = hcat(W*x,b)\n",
    "    probs = exp.(probs .- maximum(probs))\n",
    "    probs = probs ./ sum(probs,dims=1)\n",
    "    return probs\n",
    "end\n",
    "\n",
    "Random.seed!(1)\n",
    "W, b = init_params(150, 10)\n",
    "x = randn(150,32)\n",
    "y = softmax_forw(W, b, x)\n",
    "\n",
    "@info \"Testing softmax_forw\"\n",
    "@test isapprox(sum(abs2.(sum(y, dims=1) .- 1.0)), 0.0; atol=1e-10)\n",
    "@test y[42] ≈ 0.09951000272"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.2\n",
    "In this function you should firstly do the forward pass using the previous function that you implemented, after that you should calculate negative log likelihood loss:\n",
    "\n",
    "\\begin{eqnarray}{\\widehat {\\ell \\,}}(w, b \\,;x)={\\frac {-1}{n}}\\sum _{i=1}^{n} y_{i}\\ln f(x_{i}\\mid w,b )\\end{eqnarray}\n",
    "And then you should calculate the gradients of w and b and return them with the loss value.\n",
    "functions you may use: `log`, `sum`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Testing softmax_back_and_loss\n",
      "└ @ Main In[6]:23\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    softmax_back_and_loss(W, b, x, ygold)\n",
    "\n",
    "Do softmax forward pass and Return the loss and the gradients of w and b.\n",
    "\"\"\"\n",
    "function softmax_back_and_loss(W, b, x, ygold)\n",
    "    # Your code here\n",
    "    scores = softmax_forw(W, b, x)\n",
    "    loss = -sum(ygold .* log.(scores)) / size(x,2)\n",
    "    \n",
    "    reg = -1 / size(x,2) * (ygold - scores)\n",
    "    gradw = reg * x'\n",
    "    gradb = sum(reg,dims=2)\n",
    "    return loss, gradw, gradb\n",
    "end\n",
    "\n",
    "Random.seed!(1)\n",
    "x = randn(150, 32)\n",
    "ygold = zeros(10, 32)\n",
    "ygold[1, :] .= 1\n",
    "loss, gradw, gradb = softmax_back_and_loss(W, b, x, ygold)\n",
    "\n",
    "@info \"Testing softmax_back_and_loss\"\n",
    "@test loss ≈ 2.302861396444973\n",
    "@test gradw[1] ≈ 0.3196925584\n",
    "@test gradb[2] ≈ 0.100393307"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "Given model weights `W` and `b`, and one minibatch input data `x` and true labels `ygold` as input parameters, this function should display info about whether your gradient calculation procedure passes the gradient check test or not.\n",
    "Your part of this function is to implement the `numeric_gradient` function, which should calculate the numeric gradients `gw` and `gb` and return them.\n",
    "Hint: you'll need to do the calculation of the loss for each single value of the parameters twice.\n",
    "To calculate the numeric gradient of a function recall this:\n",
    "\n",
    "\\begin{eqnarray}f'(x)={\\frac {f(x+h)-f(x-h)}{2h}}\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Testing grad_check\n",
      "└ @ Main In[7]:67\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diff: 3.3840340958824455e-9\n",
      "Gradient Checking Passed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    grad_check(W, b, x, ygold)\n",
    "\n",
    "Check the accuracy of gradients of `W` and `b` which are calculated by `softmax_back_and_loss` function.\n",
    "\n",
    "This function does that by comparison with the numeric gradients.\n",
    "\"\"\"\n",
    "function grad_check(W, b, x, ygold)\n",
    "    \"\"\"\n",
    "        numeric_gradient()\n",
    "\n",
    "    Return numeric gradients of model weights `(gw, gb)`\n",
    "    \"\"\"\n",
    "    function numeric_gradient()\n",
    "        epsilon = 0.0001\n",
    "\n",
    "        gw = zeros(size(W)) # gradient of W\n",
    "        gb = zeros(size(b)) # gradient of b\n",
    "\n",
    "        # Your code here\n",
    "        \n",
    "        for i=1:length(W)\n",
    "            m_w = copy(W)\n",
    "            p_w = copy(W)    \n",
    "\n",
    "            m_w[i] = W[i]- epsilon\n",
    "            p_w[i] = W[i]+ epsilon\n",
    "            j1, _, _=softmax_back_and_loss(m_w, b, x, ygold)\n",
    "            j2, _, _=softmax_back_and_loss(p_w, b, x, ygold)\n",
    "            gw[i] = (j2-j1) / (2*epsilon)\n",
    "        end\n",
    "        for i=1:length(b)\n",
    "            m_b = copy(b)\n",
    "            p_b = copy(b)\n",
    "            \n",
    "            m_b[i] = b[i]- epsilon\n",
    "            p_b[i] = b[i]+ epsilon\n",
    "            j1, _, _=softmax_back_and_loss(W, m_b, x, ygold)\n",
    "            j2, _, _=softmax_back_and_loss(W, p_b, x, ygold)\n",
    "\n",
    "            gb[i] = (j2-j1) / (2*epsilon)\n",
    "        end\n",
    "        \n",
    "        return gw, gb\n",
    "    end\n",
    "\n",
    "    _,gradW,gradB = softmax_back_and_loss(W, b, x, ygold)\n",
    "    gw, gb = numeric_gradient()\n",
    "\n",
    "    diff = sqrt(sum((gradW - gw) .^ 2) + sum((gradB - gb) .^ 2))\n",
    "    println(\"Diff: $diff\")\n",
    "    if diff < 1e-7\n",
    "        println(\"Gradient Checking Passed\")\n",
    "        return true\n",
    "    else\n",
    "        println(\"Diff must be < 1e-7\")\n",
    "        return false\n",
    "    end\n",
    "end\n",
    "\n",
    "Random.seed!(1)\n",
    "W, b = init_params(150, 10)\n",
    "x = randn(150, 32)\n",
    "ygold = zeros(10, 32)\n",
    "ygold[1, :] .= 1\n",
    "\n",
    "@info \"Testing grad_check\"\n",
    "@test grad_check(W, b, x, ygold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5\n",
    "Given model weights `W` and `b`, set of minibatches `data` and learning rate `lr` as input this function should iterate over\n",
    "the whole dataset and in each iteration the weights should be updated using the gradients obtained from `softmax_back_and_loss` function call.\n",
    "Remember the update step of gradient descent optimization algorithm:\n",
    "\n",
    "\\begin{eqnarray}w_{i}:=w_{i}-\\eta \\nabla w_{i}\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Testing train(W, b, data, lr=0.15) function\n",
      "└ @ Main In[8]:28\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    train(W, b, data, lr=0.15)\n",
    "\n",
    "Update the models weights `W`, `b` using the `data` with learning rate of `lr` and Return the average loss.\n",
    "\"\"\"\n",
    "function train(W, b, data, lr=0.15)\n",
    "    totalcost = 0.0\n",
    "    numins = 0\n",
    "    for (x, y) in data\n",
    "        # Your code here\n",
    "        loss, gradw, gradb = softmax_back_and_loss(W, b, x, y)\n",
    "        \n",
    "        W .= W - lr * gradw\n",
    "        b .= b - lr * gradb\n",
    "        \n",
    "        totalcost += loss * size(y,2)\n",
    "        numins += size(y,2)\n",
    "    end\n",
    "\n",
    "    avgcost = totalcost / numins\n",
    "end\n",
    "\n",
    "Random.seed!(1)\n",
    "W, b = init_params(150, 10)\n",
    "ygold = zeros(10, 32)\n",
    "ygold[1, :] .= 1\n",
    "\n",
    "@info \"Testing train(W, b, data, lr=0.15) function\"\n",
    "@test train(W, b, [(randn(150, 32), ygold) for i=1:5]) ≈ 2.037075694591626\n",
    "@test W[17] ≈ 0.00204506676004\n",
    "@test b[end] ≈ -0.0713404612941"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't touch this cell. Read it carefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Testing accuracy(ygold, ypred) function\n",
      "└ @ Main In[9]:18\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    accuracy(ygold, ypred)\n",
    "\n",
    "Return accuracy true labels (ygold) and predicted scores as input for single minibatch.\n",
    "\"\"\"\n",
    "function accuracy(ygold, ypred)\n",
    "    correct = 0.0\n",
    "    for i=1:size(ygold, 2)\n",
    "        correct += findmax(ygold[:,i]; dims=1)[2] == findmax(ypred[:, i]; dims=1)[2] ? 1.0 : 0.0\n",
    "    end\n",
    "    return correct / size(ygold, 2)\n",
    "end\n",
    "\n",
    "Random.seed!(1)\n",
    "W, b = init_params(150, 10)\n",
    "ygold = zeros(10, 32)\n",
    "\n",
    "@info \"Testing accuracy(ygold, ypred) function\"\n",
    "@test accuracy(ygold, softmax_forw(W, b, randn(150, 32))) == 0.09375"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't touch this cell. Read it carefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "main (generic function with 1 method)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function main()\n",
    "    Random.seed!(12345)\n",
    "\n",
    "    # Size of input vector (MNIST images are 28x28)\n",
    "    ninputs = 28 * 28\n",
    "\n",
    "    # Number of classes (MNIST images fall into 10 classes)\n",
    "    noutputs = 10\n",
    "\n",
    "    #### Data loading & preprocessing\n",
    "    #\n",
    "    #  In this section, we load the input and output data,\n",
    "    #  prepare data to feed into softmax model.\n",
    "    #  For softmax regression on MNIST pixels,\n",
    "    #  the input data is the images, and\n",
    "    #  the output data is the labels.\n",
    "    #  Size of xtrn: (28,28,60000)\n",
    "    #  Size of xtrn must be: (784, 60000)\n",
    "    #  Size of xtst must be: (784, 10000)\n",
    "    #  Size of ytrn must be: (10, 60000)\n",
    "    #  Size of ytst must be: (10, 10000)\n",
    "\n",
    "    xtrn,ytrn = MNIST.traindata(Float32); ytrn[ytrn.==0] .= 10\n",
    "    xtst,ytst = MNIST.testdata(Float32);  ytst[ytst.==0] .= 10\n",
    "    xtrn = reshape(xtrn, 784, 60000)\n",
    "    xtst = reshape(xtst, 784, 10000)\n",
    "\n",
    "    function to_onehot(x)\n",
    "        onehot = zeros(10, 1)\n",
    "        onehot[x, 1] = 1.0\n",
    "        return onehot\n",
    "    end\n",
    "\n",
    "    ytrn = hcat(map(to_onehot, ytrn)...)\n",
    "    ytst = hcat(map(to_onehot, ytst)...)\n",
    "\n",
    "    # STEP 1: Create minibatches\n",
    "    #   Complete the minibatch function\n",
    "    #   It takes the input matrix (X) and gold labels (Y)\n",
    "    #   returns list of tuples contain minibatched input and labels (x, y)\n",
    "    bs = 100\n",
    "    trn_data = minibatch(xtrn, ytrn, bs)\n",
    "\n",
    "    # STEP 2: Initialize parameters\n",
    "    #   Complete init_params function\n",
    "    #   It takes number of inputs and number of outputs(number of classes)\n",
    "    #   It returns randomly generated W matrix and bias vector\n",
    "    #   Sample from N(0, 0.001)\n",
    "\n",
    "    W, b = init_params(ninputs, noutputs)\n",
    "\n",
    "    # STEP 3: Implement softmax_forw and softmax_back_and_loss\n",
    "    #   softmax_forw function takes W, b, and data\n",
    "    #   calculates predicted probabilities\n",
    "    #\n",
    "    #   softmax_back_and_loss function obtains probabilites by calling\n",
    "    #   softmax_forw then calculates soft loss and gradients of W and b\n",
    "\n",
    "    # STEP 4: Gradient checking\n",
    "    #   Skip this part for the lab session.\n",
    "    #   As with any learning algorithm, you should always check that your\n",
    "    #   gradients are correct before learning the parameters.\n",
    "\n",
    "    debug = true # Turn this parameter off, after gradient checking passed\n",
    "    if debug\n",
    "        grad_check(W, b, xtrn[:, 1:100], ytrn[:, 1:100])\n",
    "    end\n",
    "\n",
    "    lr = 0.15\n",
    "\n",
    "    # STEP 5: Training\n",
    "    #   The train function takes model parameters and the data\n",
    "    #   Trains the model over minibatches\n",
    "    #   For each minibatch, first cost and gradients are calculated then model parameters are updated\n",
    "    #   train function returns the average cost per instance\n",
    "\n",
    "    for i=1:50\n",
    "        cost = train(W, b, trn_data, lr)\n",
    "        pred = softmax_forw(W, b, xtrn)\n",
    "        trnacc = accuracy(ytrn, pred)\n",
    "        pred = softmax_forw(W, b, xtst)\n",
    "        tstacc = accuracy(ytst, pred)\n",
    "        @printf(\"epoch: %d softloss: %g trn accuracy: %g tst accuracy: %g\\n\", i, cost, trnacc, tstacc)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "BoundsError: attempt to access (784, 60000)\n  at index [3]",
     "output_type": "error",
     "traceback": [
      "BoundsError: attempt to access (784, 60000)\n  at index [3]",
      "",
      "Stacktrace:",
      " [1] getindex(::Tuple, ::Int64) at .\\tuple.jl:24",
      " [2] minibatch(::Array{Float32,2}, ::Array{Float64,2}, ::Int64) at .\\In[2]:12",
      " [3] main() at .\\In[9]:42",
      " [4] top-level scope at In[10]:1",
      " [5] include_string(::Function, ::Module, ::String, ::String) at .\\loading.jl:1091",
      " [6] execute_code(::String, ::String) at C:\\Users\\burak\\.julia\\packages\\IJulia\\rWZ9e\\src\\execute_request.jl:27",
      " [7] execute_request(::ZMQ.Socket, ::IJulia.Msg) at C:\\Users\\burak\\.julia\\packages\\IJulia\\rWZ9e\\src\\execute_request.jl:86",
      " [8] #invokelatest#1 at .\\essentials.jl:710 [inlined]",
      " [9] invokelatest at .\\essentials.jl:709 [inlined]",
      " [10] eventloop(::ZMQ.Socket) at C:\\Users\\burak\\.julia\\packages\\IJulia\\rWZ9e\\src\\eventloop.jl:8",
      " [11] (::IJulia.var\"#15#18\")() at .\\task.jl:356"
     ]
    }
   ],
   "source": [
    "main()\n",
    "\n",
    "#= Example Output\n",
    "Diff: 1.8292339049184216e-9\n",
    "Gradient Checking Passed\n",
    "epoch: 1 softloss: 0.481559 trn accuracy: 0.896983 tst accuracy: 0.9064\n",
    "epoch: 2 softloss: 0.339105 trn accuracy: 0.907617 tst accuracy: 0.9119\n",
    "epoch: 3 softloss: 0.31604 trn accuracy: 0.912017 tst accuracy: 0.9142\n",
    "epoch: 4 softloss: 0.303876 trn accuracy: 0.914783 tst accuracy: 0.9156\n",
    "epoch: 5 softloss: 0.29597 trn accuracy: 0.916567 tst accuracy: 0.9172\n",
    "epoch: 6 softloss: 0.290259 trn accuracy: 0.918033 tst accuracy: 0.9187\n",
    "epoch: 7 softloss: 0.285858 trn accuracy: 0.919233 tst accuracy: 0.9198\n",
    "epoch: 8 softloss: 0.282317 trn accuracy: 0.920083 tst accuracy: 0.92\n",
    "epoch: 9 softloss: 0.279378 trn accuracy: 0.9209 tst accuracy: 0.9204\n",
    "epoch: 10 softloss: 0.276879 trn accuracy: 0.921717 tst accuracy: 0.9211\n",
    "epoch: 11 softloss: 0.274716 trn accuracy: 0.92225 tst accuracy: 0.9207\n",
    "epoch: 12 softloss: 0.272816 trn accuracy: 0.92305 tst accuracy: 0.9214\n",
    "epoch: 13 softloss: 0.271127 trn accuracy: 0.923667 tst accuracy: 0.9214\n",
    "epoch: 14 softloss: 0.269609 trn accuracy: 0.924133 tst accuracy: 0.9215\n",
    "epoch: 15 softloss: 0.268235 trn accuracy: 0.924417 tst accuracy: 0.922\n",
    "epoch: 16 softloss: 0.26698 trn accuracy: 0.9247 tst accuracy: 0.9219\n",
    "epoch: 17 softloss: 0.265828 trn accuracy: 0.924933 tst accuracy: 0.9218\n",
    "epoch: 18 softloss: 0.264764 trn accuracy: 0.92505 tst accuracy: 0.922\n",
    "epoch: 19 softloss: 0.263777 trn accuracy: 0.925367 tst accuracy: 0.9223\n",
    "epoch: 20 softloss: 0.262856 trn accuracy: 0.92575 tst accuracy: 0.9225\n",
    "epoch: 21 softloss: 0.261995 trn accuracy: 0.9263 tst accuracy: 0.9227\n",
    "epoch: 22 softloss: 0.261186 trn accuracy: 0.926567 tst accuracy: 0.9226\n",
    "epoch: 23 softloss: 0.260424 trn accuracy: 0.9269 tst accuracy: 0.9229\n",
    "epoch: 24 softloss: 0.259704 trn accuracy: 0.92715 tst accuracy: 0.9227\n",
    "epoch: 25 softloss: 0.259022 trn accuracy: 0.927367 tst accuracy: 0.9227\n",
    "epoch: 26 softloss: 0.258374 trn accuracy: 0.9275 tst accuracy: 0.9229\n",
    "epoch: 27 softloss: 0.257758 trn accuracy: 0.927767 tst accuracy: 0.923\n",
    "epoch: 28 softloss: 0.257171 trn accuracy: 0.928083 tst accuracy: 0.9229\n",
    "epoch: 29 softloss: 0.25661 trn accuracy: 0.92825 tst accuracy: 0.9231\n",
    "epoch: 30 softloss: 0.256073 trn accuracy: 0.92835 tst accuracy: 0.9229\n",
    "epoch: 31 softloss: 0.255558 trn accuracy: 0.928517 tst accuracy: 0.923\n",
    "epoch: 32 softloss: 0.255064 trn accuracy: 0.928783 tst accuracy: 0.9228\n",
    "epoch: 33 softloss: 0.254589 trn accuracy: 0.92895 tst accuracy: 0.9229\n",
    "epoch: 34 softloss: 0.254133 trn accuracy: 0.9291 tst accuracy: 0.9227\n",
    "epoch: 35 softloss: 0.253692 trn accuracy: 0.929167 tst accuracy: 0.9228\n",
    "epoch: 36 softloss: 0.253268 trn accuracy: 0.92925 tst accuracy: 0.9227\n",
    "epoch: 37 softloss: 0.252858 trn accuracy: 0.929417 tst accuracy: 0.923\n",
    "epoch: 38 softloss: 0.252462 trn accuracy: 0.929567 tst accuracy: 0.9229\n",
    "epoch: 39 softloss: 0.252078 trn accuracy: 0.929667 tst accuracy: 0.9228\n",
    "epoch: 40 softloss: 0.251707 trn accuracy: 0.929783 tst accuracy: 0.9229\n",
    "epoch: 41 softloss: 0.251347 trn accuracy: 0.929867 tst accuracy: 0.9231\n",
    "epoch: 42 softloss: 0.250998 trn accuracy: 0.930067 tst accuracy: 0.9235\n",
    "epoch: 43 softloss: 0.25066 trn accuracy: 0.9301 tst accuracy: 0.9235\n",
    "epoch: 44 softloss: 0.250331 trn accuracy: 0.930233 tst accuracy: 0.9235\n",
    "epoch: 45 softloss: 0.250011 trn accuracy: 0.930333 tst accuracy: 0.9235\n",
    "epoch: 46 softloss: 0.2497 trn accuracy: 0.9305 tst accuracy: 0.9237\n",
    "epoch: 47 softloss: 0.249397 trn accuracy: 0.930583 tst accuracy: 0.9238\n",
    "epoch: 48 softloss: 0.249102 trn accuracy: 0.9307 tst accuracy: 0.9239\n",
    "epoch: 49 softloss: 0.248815 trn accuracy: 0.93085 tst accuracy: 0.9242\n",
    "epoch: 50 softloss: 0.248535 trn accuracy: 0.930933 tst accuracy: 0.9243\n",
    "=#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.2",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 3
}
